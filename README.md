# Explainable Process Mining SaaS

> **Clarity from Complexity.**  
> This project is a production-grade, multi-tenant, explainable **process mining SaaS** that connects to common business systems (Salesforce, Marketo, HubSpot, Jira, Xero, etc.), discovers real processes using PM4Py, and explains *why* bottlenecks and delays occur using AI and SHAP-style drivers.

---

## ğŸš€ What This Project Does

- **Connects** (read-only) to popular SaaS tools and file uploads (CSV/Excel).
- **Extracts & normalises** activity logs into a unified event model.
- Uses **PM4Py** to:
  - Build event logs
  - Discover process models (Inductive Miner, DFG)
  - Compute key metrics (cycle time, rework, throughput, conformance)
- Adds an **AI explainability layer** to:
  - Identify drivers behind delays and defects
  - Explain bottlenecks (â€œWhy this bottleneck?â€)
  - Generate role-specific summaries (Analyst, COO, CFO, CEO)
- Provides a **web-based dashboard** to:
  - Explore process maps and variants
  - Filter by date/system/variant thresholds
  - Inspect provenance (how the data was transformed)
  - Export reports (PDF/CSV, and later PPTX)

---

## ğŸ§© Core Features

- ğŸ”Œ **Integration Layer**
  - Read-only connectors for systems like Salesforce, Marketo, HubSpot, Jira
  - CSV/Excel upload for other sources
  - Date-bounded extraction and schema fingerprinting

- âš™ï¸ **ETL & Normalisation**
  - AI-assisted schema mapping
  - Unified event schema (case_id, activity, timestamp, resource)
  - Schema drift detection & reporting
  - Full transformation provenance log

- ğŸ§­ **Process Mining**
  - PM4Py-based event log construction
  - Discovery via Inductive Miner & DFG
  - Variant explorer (top variants, frequencies, cycle times)
  - Performance metrics: cycle time, wait vs work, throughput, rework

- ğŸ¤– **AI Insight & Explainability**
  - SHAP-style driver analysis (global & local)
  - â€œWhy this bottleneck?â€ explanations with segment comparison
  - Counterfactual â€œwhat-ifâ€ scenarios (e.g. reduce time at a step)
  - Narrative summaries tuned per role (Analyst/COO/CFO/CEO)

- ğŸ“Š **Role-Based Dashboards**
  - **Analyst:** full detail, configuration, provenance
  - **COO:** SLAs, bottlenecks, workload distribution
  - **CFO:** cost-of-waste, ROI indicators
  - **CEO:** executive summary & top 3 improvement opportunities

- ğŸ” **Governance & Compliance**
  - Multi-tenant isolation
  - Regional data residency (EU/UK/US)
  - Provenance & audit logging
  - GDPR/UK-GDPRâ€“aligned lifecycle

---

## ğŸ—ï¸ Architecture & Tech Stack

**Frontend**
- [Astro](https://astro.build/) + React
- TypeScript
- Tailwind CSS
- D3.js / Recharts for charts and process visualisation

**Backend**
- Python 3.11+ with [FastAPI](https://fastapi.tiangolo.com/)
- Celery + Redis for async jobs
- PM4Py for process mining
- SQLAlchemy with PostgreSQL

**Data & Storage**
- DuckDB + Parquet for analytics
- PostgreSQL for metadata & auth
- MinIO / S3-compatible storage for raw extracts & artefacts

**AI & Explainability**
- OpenAI / Anthropic APIs
- Optional: local inference via Ollama
- SHAP-style driver analysis for explainability

**Infra / DevOps**
- Docker / Docker Compose for local dev
- Kubernetes (or equivalent) for production
- GitHub Actions for CI/CD
- CDN/static hosting (e.g. Cloudflare) for frontend

For detailed architecture, see:

- `system_architecture.md`
- `process_mining_architecture.md`
- `process_mining_visual_architecture_diagrams.md` (Mermaid diagrams)

---

## ğŸ“‚ Repository Structure (Proposed)

> This may evolve as the implementation progresses. Keep this section updated.

```text
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/                # FastAPI routers
â”‚   â”‚   â”œâ”€â”€ core/               # config, security, logging
â”‚   â”‚   â”œâ”€â”€ models/             # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ schemas/            # Pydantic models
â”‚   â”‚   â”œâ”€â”€ services/           # business logic
â”‚   â”‚   â”œâ”€â”€ workers/            # Celery tasks (ETL, mining, AI)
â”‚   â”‚   â””â”€â”€ main.py             # FastAPI entrypoint
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ layouts/
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ k8s/                    # deployment manifests (optional)
â”‚   â””â”€â”€ Makefile
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README.md               # (this file)
â”‚   â”œâ”€â”€ agent.md
â”‚   â”œâ”€â”€ coding_plan.md
â”‚   â”œâ”€â”€ coding_guidelines.md
â”‚   â”œâ”€â”€ ui_ux_guidelines.md
â”‚   â”œâ”€â”€ data_model_and_storage.md
â”‚   â”œâ”€â”€ api_design_backend.md
â”‚   â”œâ”€â”€ process_mining_engine.md
â”‚   â”œâ”€â”€ ai_explainability_and_reports.md
â”‚   â”œâ”€â”€ security_and_compliance_dev.md
â”‚   â”œâ”€â”€ testing_and_quality.md
â”‚   â”œâ”€â”€ deployment_and_devops.md
â”‚   â”œâ”€â”€ process_mining_saas_brainstorm.md
â”‚   â”œâ”€â”€ process_mining_pricing_packaging.md
â”‚   â”œâ”€â”€ process_mining_ux_wireframes.md
â”‚   â”œâ”€â”€ process_mining_value_proposition.md
â”‚   â”œâ”€â”€ process_mining_gtm_outreach_plan.md
â”‚   â”œâ”€â”€ process_mining_data_governance_compliance.md
â”‚   â””â”€â”€ process_mining_product_marketing_narrative.md
â”‚
â””â”€â”€ error_log.md


---

ğŸ§  For AI Coding Agents

This repository is designed to be agent-friendly.

Start with agent.md
This file explains your mission as an AI coding agent, required knowledge, and how to follow the projectâ€™s rules.

Follow the implementation sequence in:

coding_plan.md


Adhere strictly to:

coding_guidelines.md

ui_ux_guidelines.md

security_and_compliance_dev.md

testing_and_quality.md


Respect architecture and data model:

system_architecture.md

data_model_and_storage.md

process_mining_engine.md

api_design_backend.md

ai_explainability_and_reports.md


Log bugs and incidents in:

error_log.md




---

ğŸ§‘â€ğŸ’» Getting Started (Local Development)

1. Prerequisites

Docker & Docker Compose

Node.js (LTS) + pnpm / npm / yarn

Python 3.11+


2. Clone & Bootstrap

git clone <your-repo-url>.git
cd <your-repo-folder>

Install backend dependencies (example with uv or poetry/pip):

cd backend
# e.g. using uv or poetry or pip:
# uv pip install -r requirements.txt
# or
# poetry install

Install frontend dependencies:

cd ../frontend
npm install   # or pnpm install / yarn install

3. Environment Configuration

Create .env files from provided examples (if present):

cp backend/.env.example backend/.env
cp frontend/.env.example frontend/.env

Set values for:

Database connection (PostgreSQL)

Redis URL

Object storage credentials (MinIO)

AI API keys (OpenAI, Anthropic, etc.)


4. Run with Docker Compose

From the project root:

cd infra
docker-compose up --build

This should start:

Backend API (FastAPI)

Frontend (Astro/React)

PostgreSQL

Redis

MinIO


Check:

API health: http://localhost:<api-port>/health

Frontend: http://localhost:<frontend-port>/



---

ğŸ§ª Testing

Backend

cd backend
pytest

Frontend

cd frontend
npm test

CI (via GitHub Actions) should:

Lint & type-check

Run tests

Build Docker images


For full strategy see:

testing_and_quality.md

deployment_and_devops.md



---

ğŸ” Security & Compliance

This project is designed with regulated industries in mind (banking, pharma, manufacturing, etc.).

Key principles:

Read-only integrations

Tenant-level data isolation

Provenance & audit logs for all analyses

Respect for data residency and GDPR/UK-GDPR


Developer-facing rules live in:

security_and_compliance_dev.md

process_mining_data_governance_compliance.md



---

ğŸ—ºï¸ Roadmap (High-Level)

See coding_plan.md for detailed milestones. Broadly:

1. CSV-only MVP (upload â†’ event log â†’ basic process map)


2. SaaS connectors + provenance


3. Full PM4Py analytics & variant explorer


4. AI explainability (SHAP-like drivers) + â€œWhy this bottleneck?â€


5. Role-based dashboards & report exports


6. Hardened multi-tenant SaaS with CI/CD & monitoring




---

ğŸ¤ Contributing

Please follow coding_guidelines.md and ui_ux_guidelines.md.

Keep error_log.md updated with significant issues and resolutions.

Update documentation when APIs or models change.



---

ğŸ“œ Licence

> Add your chosen licence here (e.g. MIT, Apache 2.0).
Until specified, treat this as proprietary / internal.




---

ğŸ“£ Contact & Credits

This project originates from a vision to make explainable process mining accessible, transparent, and affordable â€” especially for organisations that find Signavio/ARIS too complex or expensive.

Strategic & product design: @<your-handle>

Architecture & docs: see /docs folder.


â€œProcess mining made human.â€

If youâ€™d like, I can also generate a short `CONTRIBUTING.md` to complement this and set expectations for PRs, branches, and coding standards.
